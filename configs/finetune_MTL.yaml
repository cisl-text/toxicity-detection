train:
  # earlystopping + save
  patience: 5
  delta: 0
  savePretrained: True # 只保存pretrain模型
  saveDir: "./models/finetuneRobertaInter"
  # MODEL
  # "./models/hateBERT
  model: "roberta-base"
  tokenizer: "roberta-base"
  # training setting
  epoch: 32
  batch_size: 4
  cuda: '0'
  lr: 0.00001
  bertadam: True
  warmup_proportion: 0.1 # defalut: 0
  weight_decay: False
  # Dataset Configuration
  dataset:
    # GabHateCorpus
    # ImplicitHateCorpus
    # SBIC
    shuffle: False
    splitRatio: 0.8
    name: 'SBIC'
    label_names: ["non-inter","inter"]
    correction_dir:
    correction_size:
    mode: 'international'
  # Loader Configuration
  num_workers: 0

  # eval_all
  evalAll: True  # If bertadam, then set correct_bias = False
