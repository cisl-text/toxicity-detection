train:
  # earlystopping + save
  patience: 5
  delta: 0
  saveDir: "./models/multiview-1"
  # training setting
  epoch: 32
  batch_size: 42
  cuda: "5,6,7"
  lr: 0.00001
  bertadam: True
  warmup_proportion: 0.1 # defalut: 0
  weight_decay: False
  dataset:
    tokenizer: "roberta-base"
    # ImplicitHateCorpus
    shuffle: False
    splitRatio: 0.8
    name: 'ImplicitHateCorpus'
    # Loader Configuration
    num_workers: 4
    add_dep: True
model:
  bert_model: "roberta-base"
  word_emb_dim: 768
  lstm_hid_dim: 300
  dropout_rate: 0.5