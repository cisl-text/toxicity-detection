{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    GPTNeoForCausalLM,\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"/dev-data/ybshu/plms/gpt-neo-1.3B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name_or_path)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import DictWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Writer():\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = model.device\n",
    "    \n",
    "    def write_one(self, prefix, prompt = \"I think\", length = 128):\n",
    "        encoded_prompt = tokenizer.encode(\" \".join([prefix, prompt]), add_special_tokens=False, return_tensors=\"pt\")\n",
    "        encoded_prompt = encoded_prompt.to(model.device)\n",
    "\n",
    "        input_ids = encoded_prompt\n",
    "        eos_token = tokenizer.eos_token\n",
    "        eos_token_id = tokenizer.eos_token_id\n",
    "        if True:\n",
    "            length = length\n",
    "            temperature = 1\n",
    "            num_beam = 5\n",
    "            k = 50\n",
    "            p = 1\n",
    "            repetition_penalty = 1\n",
    "            do_sample=True\n",
    "            num_return_sequence = 3\n",
    "\n",
    "        output_sequences = model.generate(\n",
    "            input_ids = input_ids,\n",
    "            max_length = len(encoded_prompt[0]) + length,\n",
    "            temperature = temperature,\n",
    "            #num_beam = num_beam,\n",
    "            top_k = k,\n",
    "            top_p = p,\n",
    "            repetition_penalty = repetition_penalty,\n",
    "            do_sample = do_sample,\n",
    "            num_return_sequence = num_return_sequence,\n",
    "            pad_token_id = eos_token_id\n",
    "        )\n",
    "\n",
    "\n",
    "        generated_sequences = []\n",
    "        stop_token = tokenizer.eos_token\n",
    "        # stop_token = \"\\n\"\n",
    "\n",
    "        for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
    "            #print(f\"=== GENERATED SEQUENCE {generated_sequence_idx + 1} ===\")\n",
    "            generated_sequence = generated_sequence.tolist()\n",
    "\n",
    "            # Decode text\n",
    "            text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
    "\n",
    "            # Remove all text after the stop token\n",
    "            text = text[: text.find(stop_token) if stop_token else None]\n",
    "\n",
    "            # Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing\n",
    "            total_sequence = (\n",
    "                prompt + text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n",
    "            )\n",
    "\n",
    "            generated_sequences.append(total_sequence)\n",
    "            #print(total_sequence)\n",
    "        return generated_sequences\n",
    "\n",
    "\n",
    "    def write_and_save(self, prefix_source, target, prompt = \"I think\", length = 128):\n",
    "        \n",
    "        f = open(prefix_source, \"r\")\n",
    "        field_names = ['prefix','prompt','generation']\n",
    "        \n",
    "        for line in tqdm(f.readlines()):\n",
    "            prefix = line.strip()\n",
    "            generation = self.write_one(prefix, prompt, length)\n",
    "            if len(generation)==0:\n",
    "                continue\n",
    "            row = {\"prefix\":prefix, \"prompt\":prompt, \"generation\":generation[0]}\n",
    "            with open(target, \"a\") as out_f:\n",
    "                dictwriter_object = DictWriter(out_f, fieldnames=field_names)\n",
    "                dictwriter_object.writerow(row)\n",
    "\n",
    "        f.close()\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"I am willing to call black people nigga. \"\n",
    "prompt = \"I hate\"\n",
    "\n",
    "encoded_prompt = tokenizer.encode(\" \".join([prefix, prompt]), add_special_tokens=False, return_tensors=\"pt\")\n",
    "encoded_prompt = encoded_prompt.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encoded_prompt\n",
    "eos_token = tokenizer.eos_token\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "length = 256\n",
    "temperature = 1\n",
    "num_beam = 5\n",
    "k = 50\n",
    "p = 1\n",
    "repetition_penalty = 1\n",
    "do_sample=True\n",
    "num_return_sequence = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sequences = model.generate(\n",
    "    input_ids = input_ids,\n",
    "    max_length = len(encoded_prompt[0]) + length,\n",
    "    temperature = temperature,\n",
    "    #num_beam = num_beam,\n",
    "    top_k = k,\n",
    "    top_p = p,\n",
    "    repetition_penalty = repetition_penalty,\n",
    "    do_sample = do_sample,\n",
    "    num_return_sequence = num_return_sequence,\n",
    "    pad_token_id = eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GENERATED SEQUENCE 1 ===\n",
      "I hate that word.  I hate to use that word.  I don't mean to say that nigga is a racist term.  I know people like that.\n"
     ]
    }
   ],
   "source": [
    "generated_sequences = []\n",
    "# stop_token = tokenizer.eos_token\n",
    "stop_token = \"\\n\"\n",
    "\n",
    "for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
    "    print(f\"=== GENERATED SEQUENCE {generated_sequence_idx + 1} ===\")\n",
    "    generated_sequence = generated_sequence.tolist()\n",
    "\n",
    "    # Decode text\n",
    "    text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    # Remove all text after the stop token\n",
    "    text = text[: text.find(stop_token) if stop_token else None]\n",
    "\n",
    "    # Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing\n",
    "    total_sequence = (\n",
    "        prompt + text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n",
    "    )\n",
    "\n",
    "    generated_sequences.append(total_sequence)\n",
    "    print(total_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = Writer(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explicit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1089/1089 [56:03<00:00,  3.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "implicit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3572/7100 [3:04:26<2:49:36,  2.88s/it]"
     ]
    }
   ],
   "source": [
    "file_name_list = [\"explicit\", \"implicit\", \"non_toxic\"]\n",
    "source_path = \"/dev-data/ybshu/code/research/toxicity-detection/data/ImplicitHate/\"\n",
    "target_path = \"/dev-data/ybshu/code/research/toxicity-detection/data/ImplicitHate/augment/\"\n",
    "\n",
    "prompt = \"I think\"\n",
    "length = 128\n",
    "\n",
    "for file_name in file_name_list:\n",
    "    print(file_name)\n",
    "    source_file_path = os.path.join(source_path, file_name) + \".txt\"\n",
    "    target_file_path = os.path.join(target_path, file_name) + \".csv\"\n",
    "    writer.write_and_save(source_file_path, target_file_path, prompt, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"/dev-data/ybshu/code/research/toxicity-detection/data/ImplicitHate/augment/implicit_0_3610.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.iloc[:3611]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"/dev-data/ybshu/code/research/toxicity-detection/data/ImplicitHate/augment/implicit_3610.csv\", header=None, index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('py38_torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1b405c7b78782f92455a921167f9395958ab1fb711325fa587b618dc59c97f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
